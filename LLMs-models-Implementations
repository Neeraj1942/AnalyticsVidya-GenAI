Transformers: A type of neural network architecture that uses self-attention to process sequences of data efficiently, capturing relationships between elements regardless of their distance.
LLMs (Large Language Models): Language models built using transformers and trained on massive text corpora to generate, understand, or analyze human-like text.

Chinchilla Scaling Law :
Law : smaller models can outperform larger ones if they are trained on more data.Other models like Gopher, GPT-3, and MT-NLG 530B have significantly more parameters but were trained on relatively fewer tokens, suggesting that these models may not have fully optimized their compute potential.

Model Size (N): The number of parameters in the model.
Training Tokens (D): The total number of tokens used during training.
Computational Cost (C): The total compute resources allocated for training, usually measured in FLOPs (floating point operations per second).

(Conclusion of Chinchilla showed that smaller models trained on more data outperform larger models trained on less data.)

GGUF → the model file itself
GGML → the library that reads the model and runs it on CPU efficiently

GGUF - GGML Unified Format - A model file format used to store LLM weights, configuration, and metadata in one unified, ready-to-load file.
GGML - Georgi’s General Matrix Library - A lightweight CPU inference library optimized for running LLMs efficiently, especially with quantized weights.

GGUF is a standardized, inference-ready model file (weights + config + metadata,
Python checkpoints (.pt / .pth) are PyTorch files mainly for training/fine-tuning that store raw weights and optionally optimizer states.

TPU = Tensor Processing Uni
PUs < GPUs < TPUs in deep learning performance.


Architecture of a LLM ->
The embedding layer converts each word in the input text into a high-dimensional vector representation. These embeddings capture semantic and syntactic information about the words and help the model to understand the context.
The feedforward layers of Large Language Models have multiple fully connected layers that apply nonlinear transformations to the input embeddings. These layers help the model learn higher-level abstractions from the input text.
The recurrent layers of LLMs interpret information from the input text sequentially. These layers hold a hidden state that updates at each time step, enabling the model to capture dependencies between words in a sentence.
The attention mechanism is another important part of LLMs, which allows the model to focus selectively on different parts of the input text. This self-attention helps the model attend to the input text’s most relevant parts and generate more accurate predictions.

Llama 3.1 ->
!pip install --upgrade transformers
!pip install --upgrade torch
!pip install --upgrade huggingface_hub==0.36.0

 hf auth login ( in the terminal)

import transformers
import torch

# Define the model ID for LLaMA 3.1
model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"

# Initialize the text generation pipeline with LLaMA 3.1 model
llama3 = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},  # Using bfloat16 for efficient computation
    device_map="auto",  # Automatically maps model to available GPU or CPU
)

Create LLaMA 3.1 Completion Access Function
def get_completion_llama(prompt, model_pipeline=llama3):
    messages = [{"role": "user", "content": prompt}]
    response = model_pipeline(
        messages,
        max_new_tokens=2000
    )
    return response[0]["generated_text"][-1]['content']
Let’s try out Llama 3.1


response = get_completion_llama(prompt='Explain Generative AI in 2 bullet points')
display(Markdown(response))

or try 



from transformers import pipeline

model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
llama3 = pipeline(
    "text-generation",
    model=model_id,
    use_auth_token="my token,
)


GPT 4o ->
import openai
from IPython.display import HTML, Markdown, display
openai.api_key = openai_key

def get_completion_gpt(prompt, model="gpt-4o-mini"):

    messages = [{"role": "user", "content": prompt}]

    response = openai.chat.completions.create(

        model=model,

        messages=messages,

        temperature=0.0, # degree of randomness of the model's output

    )

    return response.choices[0].message.content

response = get_completion_gpt(prompt='Explain Generative AI in 2 bullet points')
display(Markdown(response))

Gemma 2 ->
Install Necessary Libraries

!pip install -q  -U transformers accelerate bitsandbytes huggingface_hub
 Import Necessary Libraries

import torch

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
Set-up Quantization Configuration

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)
Load Tokenizer and Model ->

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b-it", device="cuda")

model = AutoModelForCausalLM.from_pretrained(

    "google/gemma-2-9b-it",

    quantization_config=quantization_config,

    device_map="cuda")
Prepare Input Text and Tokenize ->

input_text = "For the below sentence extract the names and \

organizations in a json format\nElon Musk is the CEO of SpaceX"

input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")
Generate Output

outputs = model.generate(**input_ids, max_length = 512)
Decode and Print Output

print(tokenizer.decode(outputs[0], skip_special_tokens=True))



Claude 3.5 Sonnet ->

Installation of the Anthropic Python Package

pip install anthropic
Import the Anthropic Module

import anthropic
Create an Instance of the Anthropic API Client

client = anthropic.Anthropic(api_key='your_api_key_here')  

# Define a customer support inquiry 

customer_message = "Hi, I need help with resetting my password. Can you guide me?"

# Send the customer support inquiry to the Claude 

model response = client.messages.create( model="claude-3-5-sonnet-20240620", max_tokens=150, messages=[{"role": "user", "content": customer_message}] ) 

# Print the response from the model 

print("AI Response:", response['completion'])


SLM (Specialized Language Models) -> Can outperform LLMs in specific, well-defined tasks within their domain of expertise.



ATTENTION MECHANISMS ->
Breaking Down the Input: Let’s say you have a bunch of words (or any kind of data) that you want the computer to understand. First, it breaks down this input into smaller pieces, like individual words.
Picking Out Important Bits: Then, it looks at these pieces and decides which ones are the most important. It does this by comparing each piece to a question or ‘query’ it has in mind.
Assigning Importance: Each piece gets a score based on how well it matches the question. The higher the score, the more important that piece is.
Focusing Attention: After scoring each piece, it figures out how much attention to give to each one. Pieces with higher scores get more attention, while less important ones get less attention.
Putting It All Together: Finally, it adds up all the pieces, but gives more weight to the important ones. This way, the computer gets a clearer picture of what’s most important in the input.


We had encoder =decoder model ion 2015 
2 rnn/lstm -> 
ChatGPT said:
We need two models because the encoder reads and understands the entire input sentence, while the decoder uses that understanding to generate the output sentence. 
One model can’t do both tasks at the same time.











