Transformers: A type of neural network architecture that uses self-attention to process sequences of data efficiently, capturing relationships between elements regardless of their distance.
LLMs (Large Language Models): Language models built using transformers and trained on massive text corpora to generate, understand, or analyze human-like text.

Architecture of a LLM ->
The embedding layer converts each word in the input text into a high-dimensional vector representation. These embeddings capture semantic and syntactic information about the words and help the model to understand the context.
The feedforward layers of Large Language Models have multiple fully connected layers that apply nonlinear transformations to the input embeddings. These layers help the model learn higher-level abstractions from the input text.
The recurrent layers of LLMs interpret information from the input text sequentially. These layers hold a hidden state that updates at each time step, enabling the model to capture dependencies between words in a sentence.
The attention mechanism is another important part of LLMs, which allows the model to focus selectively on different parts of the input text. This self-attention helps the model attend to the input text’s most relevant parts and generate more accurate predictions.

Llama 3.1 ->
!pip install --upgrade transformers
!pip install --upgrade torch
!pip install --upgrade huggingface_hub==0.36.0

 hf auth login ( in the terminal)

import transformers
import torch

# Define the model ID for LLaMA 3.1
model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"

# Initialize the text generation pipeline with LLaMA 3.1 model
llama3 = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},  # Using bfloat16 for efficient computation
    device_map="auto",  # Automatically maps model to available GPU or CPU
)

Create LLaMA 3.1 Completion Access Function
def get_completion_llama(prompt, model_pipeline=llama3):
    messages = [{"role": "user", "content": prompt}]
    response = model_pipeline(
        messages,
        max_new_tokens=2000
    )
    return response[0]["generated_text"][-1]['content']
Let’s try out Llama 3.1


response = get_completion_llama(prompt='Explain Generative AI in 2 bullet points')
display(Markdown(response))

or try 



from transformers import pipeline

model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"
llama3 = pipeline(
    "text-generation",
    model=model_id,
    use_auth_token="my token,
)


GPT 4o ->
import openai
from IPython.display import HTML, Markdown, display
openai.api_key = openai_key

def get_completion_gpt(prompt, model="gpt-4o-mini"):

    messages = [{"role": "user", "content": prompt}]

    response = openai.chat.completions.create(

        model=model,

        messages=messages,

        temperature=0.0, # degree of randomness of the model's output

    )

    return response.choices[0].message.content

response = get_completion_gpt(prompt='Explain Generative AI in 2 bullet points')
display(Markdown(response))

Gemma 2 ->
Install Necessary Libraries

!pip install -q  -U transformers accelerate bitsandbytes huggingface_hub
 Import Necessary Libraries

import torch

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
Set-up Quantization Configuration

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)
Load Tokenizer and Model ->

tokenizer = AutoTokenizer.from_pretrained("google/gemma-2-9b-it", device="cuda")

model = AutoModelForCausalLM.from_pretrained(

    "google/gemma-2-9b-it",

    quantization_config=quantization_config,

    device_map="cuda")
Prepare Input Text and Tokenize ->

input_text = "For the below sentence extract the names and \

organizations in a json format\nElon Musk is the CEO of SpaceX"

input_ids = tokenizer(input_text, return_tensors="pt").to("cuda")
Generate Output

outputs = model.generate(**input_ids, max_length = 512)
Decode and Print Output

print(tokenizer.decode(outputs[0], skip_special_tokens=True))



Claude 3.5 Sonnet ->

Installation of the Anthropic Python Package

pip install anthropic
Import the Anthropic Module

import anthropic
Create an Instance of the Anthropic API Client

client = anthropic.Anthropic(api_key='your_api_key_here')  

# Define a customer support inquiry 

customer_message = "Hi, I need help with resetting my password. Can you guide me?"

# Send the customer support inquiry to the Claude 

model response = client.messages.create( model="claude-3-5-sonnet-20240620", max_tokens=150, messages=[{"role": "user", "content": customer_message}] ) 

# Print the response from the model 

print("AI Response:", response['completion'])




















