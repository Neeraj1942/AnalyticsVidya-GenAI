This is the learning path for GenAI

Bag of words (BOW) and TF-IDF : Helps convert text sentences to numeric vectors

1. Bag of words (BOW) : 
example : 
Review 1: This movie is very scary and long
Review 2: This movie is not scary and is slow
Review 3: This movie is spooky and good

the words are -> 11 words: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’
https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/BoWBag-of-Words-model-2.png
Vector-of-Review1: [1 1 1 1 1 1 1 0 0 0 0]
Vector-of-Review2: [1 1 2 0 0 1 1 0 1 0 0]
Vector-of-Review3: [1 1 1 0 0 0 1 0 0 1 1]

Drawbacks ->
1. If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too.
2. Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid)
3. We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text.

Limitations ->
No Word Order: It doesn’t care about the order of words, missing out on how words work together.
Ignores Context: It doesn’t understand the meaning of words based on the words around them.
Always Same Length: It always represents text in the same way, which can be limiting for different types of text.
Lots of Words: It needs to know every word in a language, which can be a huge list to handle.
No Meanings: It doesn’t understand what words mean, only how often they appear, so it can’t grasp synonyms or different word forms.

Term frequency - Inverse Document Frequency(TF-IDF) -> is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus
 It is a measure of how frequently a term, t, appears in a document, d: 
Here, in the numerator, n is the number of times the term “t” appears in the document “d”. Thus, each document and term would have its own TF value.

1. Term Frequency (TF)
   TF(t, d) = f(t, d) / |d|
   # f(t, d) = frequency of term t in document d
   # |d| = total number of terms in document d
   # Measures how common a term is in a specific document

2. Inverse Document Frequency (IDF)
   IDF(t) = log(N / df(t))
   # N = total number of documents in the corpus
   # df(t) = number of documents containing term t
   # Measures how rare/informative a term is across the corpus
   # log is used to dampen the effect of very rare terms

3. TF-IDF
   TF-IDF(t, d) = TF(t, d) * IDF(t)
   # Combines local importance (TF) and global rarity (IDF)
   # Higher TF-IDF → more important term for this document

Example of these steps ->
TF(‘movie’) = 1/8
TF(‘is’) = 2/8 = 1/4
TF(‘very’) = 0/8 = 0
TF(‘scary’) = 1/8
TF(‘and’) = 1/8
TF(‘long’) = 0/8 = 0
TF(‘not’) = 1/8
TF(‘slow’) = 1/8
TF( ‘spooky’) = 0/8 = 0
TF(‘good’) = 0/8 = 0

IDF(‘movie’, ) = log(3/3) = 0
IDF(‘is’) = log(3/3) = 0
IDF(‘not’) = log(3/1) = log(3) = 0.48
IDF(‘scary’) = log(3/2) = 0.18
IDF(‘and’) = log(3/3) = 0
IDF(‘slow’) = log(3/1) = 0.48

TF-IDF(‘movie’, Review 2) = 1/8 * 0 = 0
TF-IDF(‘is’, Review 2) = 1/4 * 0 = 0
TF-IDF(‘not’, Review 2) = 1/8 * 0.48 = 0.06
TF-IDF(‘scary’, Review 2) = 1/8 * 0.18 = 0.023
TF-IDF(‘and’, Review 2) = 1/8 * 0 = 0
TF-IDF(‘slow’, Review 2) = 1/8 * 0.48 = 0.06

Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.
Bag of Words vectors are easy to interpret. However, TF-IDF usually performs better in machine learning models.

Main approach : creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in


Word Embeddings -
1. Frequency-based Embedding 
2. Prediction-based Embedding

 1. Frequency-based Embedding
a. Count Vector
b. TF-IDF Vector
c. Co-Occurrence Vector

a. Count vector ->
D1: He is a lazy boy. She is also lazy.
D2: Neeraj is a lazy person.
The dictionary created may be a list of unique tokens(words) in the corpus =[‘He’,’She’,’lazy’,’boy’,’Neeraj’,’person’]

Here, D=2, N=6
He	She	lazy	boy	Neeraj	person
D1	1	1	2	1	0	0
D2	0	0	1	0	1	1

b. TD-IDF ->
TF is fraction , IDF is log(N/n) ; N is all documents, and n is times the word appreared in how many documents
TF-IDF : TF * IDF ; this helps to penalize the most common words like a,an,the,is thye etc , because N/n is always 1 , log(1) =0

c. Co-Occurrence Vector ->
1. Co-occurence 
2. Context Window

Corpus = He is not lazy. He is intelligent. He is smart
 	He	is	not	lazy	intelligent	smart
He	0	4	2	1	2	1
is	4	0	1	2	2	1
not	2	1	0	1	0	0
lazy	1	2	1	0	0	0
intelligent	2	2	0	0	0	0
smart	1	1	0	0	0	0

the whole sentence is considered on line of tokens where, going up and down is counted (window size : 2 - left or right of the line )
line : He is not lazy. He is intelligent. He is smart

Issues : It requires huge memory to store the co-occurrence matrix.


2. Prediction based embeddings ->
This section assumes that you have a working knowledge of how a neural network works and the mechanisms by which weights in an NN are updated

Word2Vec ->

Word2vec is not a single algorithm but a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model

softmax example ->
Input scores:
[1.0, 2.0, 3.0]
Exponentiate:
[e¹, e², e³] ≈ [2.72, 7.39, 20.09]
Sum:
2.72 + 7.39 + 20.09 = 30.20
Normalized:
[0.09, 0.24, 0.67]
So softmax outputs:
~9%, 24%, 67%

CBOW ->
EX:
| Input  | Output | Hey | This | is | sample | corpus | using | only | one | context | word |
| ------ | ------ | --- | ---- | -- | ------ | ------ | ----- | ---- | --- | ------- | ---- |
| Hey    | this   | 1   | 0    | 0  | 0      | 0      | 0     | 0    | 0   | 0       | 0    |
| this   | hey    | 0   | 1    | 0  | 0      | 0      | 0     | 0    | 0   | 0       | 0    |
| is     | this   | 0   | 0    | 1  | 0      | 0      | 0     | 0    | 0   | 0       | 0    |
| is     | sample | 0   | 0    | 1  | 0      | 0      | 0     | 0    | 0   | 0       | 0    |
| sample | is     | 0   | 0    | 0  | 1      | 0      | 0     | 0    | 0   | 0       | 0    |

(1*10) * (10*4)  -> INPUT LAYER * WEIGHTS ( CONSIDERING ONLY 1 WORD OR ONE-HOT EMBEDDING) -> STEP1
STEP1* OUTPUT WEIGHTS (4*10) -> OUTPUT OF (1*10)   

SIMILARLY IF WE HAVE 3 WORDS -> 3*10 MUL WEIGHTS (10*4) -> 3*4 -> THEN THE AVERAGE IS CALCULATED.


INPUT WEIGHTS (10*4)
OUPUT WEIGHTS (4*10)
AS OUR OUTPUT IS FIXED TO SIZE (1*10)

THIS IS ->
CBOW takes the average of the context of a word (as seen above in calculation of hidden activation). For example, Apple can be both a fruit and a company but CBOW takes an average of both the contexts and places it in between a cluster for fruits and companies.
Training a CBOW from scratch can take forever if not properly optimized.

SKIP - GRAM MODEL ->
WE USE 2 ERROS / OR COMPARE 2 WORDS IN THE SAME CORPUS EX:APPLE , THEN WE GET THE PROBABILTY OF THAT WORD BEING OCCURED IN THE CERTAIN MEANING ?
| Input | Context1 | Context2  |
| ----- | -------- | --------- |
| Hey   | this     | spadding> |
| this  | Hey      | is        |
| is    | this     | sample    |
| ...   | ...      | ...       |

NOTE : Skip-gram predicts multiple context words for each target word (if window size = 2, it predicts 2 context words per target word), 
but fundamentally it's still one target word at a time

| Model     | Input                      | Output                       |
| --------- | -------------------------- | ---------------------------- |
| CBOW      | All context words together | Single target word           |
| Skip-gram | Single target word         | Each context word separately |

CBOW: Uses multiple context words together to predict a single target word.
Skip-gram: Uses a single target word to predict each surrounding context word separately.

EXAMPLE :
Sentence: I love pizza
CBOW:
Input (context): I, pizza → Output (target): love
Skip-gram:
Input (target): love → Outputs (contexts): I, pizza



