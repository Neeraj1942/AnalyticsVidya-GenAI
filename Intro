This is the learning path for GenAI

Bag of words (BOW) and TF-IDF : Helps convert text sentences to numeric vectors

1. Bag of words (BOW) : 
example : 
Review 1: This movie is very scary and long
Review 2: This movie is not scary and is slow
Review 3: This movie is spooky and good

the words are -> 11 words: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’
https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/BoWBag-of-Words-model-2.png
Vector-of-Review1: [1 1 1 1 1 1 1 0 0 0 0]
Vector-of-Review2: [1 1 2 0 0 1 1 0 1 0 0]
Vector-of-Review3: [1 1 1 0 0 0 1 0 0 1 1]

Drawbacks ->
1. If the new sentences contain new words, then our vocabulary size would increase and thereby, the length of the vectors would increase too.
2. Additionally, the vectors would also contain many 0s, thereby resulting in a sparse matrix (which is what we would like to avoid)
3. We are retaining no information on the grammar of the sentences nor on the ordering of the words in the text.

Limitations ->
No Word Order: It doesn’t care about the order of words, missing out on how words work together.
Ignores Context: It doesn’t understand the meaning of words based on the words around them.
Always Same Length: It always represents text in the same way, which can be limiting for different types of text.
Lots of Words: It needs to know every word in a language, which can be a huge list to handle.
No Meanings: It doesn’t understand what words mean, only how often they appear, so it can’t grasp synonyms or different word forms.

Term frequency - Inverse Document Frequency(TF-IDF) -> is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus
 It is a measure of how frequently a term, t, appears in a document, d: 
Here, in the numerator, n is the number of times the term “t” appears in the document “d”. Thus, each document and term would have its own TF value.

1. Term Frequency (TF)
   TF(t, d) = f(t, d) / |d|
   # f(t, d) = frequency of term t in document d
   # |d| = total number of terms in document d
   # Measures how common a term is in a specific document

2. Inverse Document Frequency (IDF)
   IDF(t) = log(N / df(t))
   # N = total number of documents in the corpus
   # df(t) = number of documents containing term t
   # Measures how rare/informative a term is across the corpus
   # log is used to dampen the effect of very rare terms

3. TF-IDF
   TF-IDF(t, d) = TF(t, d) * IDF(t)
   # Combines local importance (TF) and global rarity (IDF)
   # Higher TF-IDF → more important term for this document

Example of these steps ->
TF(‘movie’) = 1/8
TF(‘is’) = 2/8 = 1/4
TF(‘very’) = 0/8 = 0
TF(‘scary’) = 1/8
TF(‘and’) = 1/8
TF(‘long’) = 0/8 = 0
TF(‘not’) = 1/8
TF(‘slow’) = 1/8
TF( ‘spooky’) = 0/8 = 0
TF(‘good’) = 0/8 = 0

IDF(‘movie’, ) = log(3/3) = 0
IDF(‘is’) = log(3/3) = 0
IDF(‘not’) = log(3/1) = log(3) = 0.48
IDF(‘scary’) = log(3/2) = 0.18
IDF(‘and’) = log(3/3) = 0
IDF(‘slow’) = log(3/1) = 0.48

TF-IDF(‘movie’, Review 2) = 1/8 * 0 = 0
TF-IDF(‘is’, Review 2) = 1/4 * 0 = 0
TF-IDF(‘not’, Review 2) = 1/8 * 0.48 = 0.06
TF-IDF(‘scary’, Review 2) = 1/8 * 0.18 = 0.023
TF-IDF(‘and’, Review 2) = 1/8 * 0 = 0
TF-IDF(‘slow’, Review 2) = 1/8 * 0.48 = 0.06

Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.
Bag of Words vectors are easy to interpret. However, TF-IDF usually performs better in machine learning models.

Main approach : creating a representation for words that capture their meanings, semantic relationships and the different types of contexts they are used in


Word Embeddings -
1. Frequency-based Embedding 
2. Prediction-based Embedding

 1. Frequency-based Embedding
a. Count Vector
b. TF-IDF Vector
c. Co-Occurrence Vector

a. Count vector ->
D1: He is a lazy boy. She is also lazy.
D2: Neeraj is a lazy person.
The dictionary created may be a list of unique tokens(words) in the corpus =[‘He’,’She’,’lazy’,’boy’,’Neeraj’,’person’]

Here, D=2, N=6
He	She	lazy	boy	Neeraj	person
D1	1	1	2	1	0	0
D2	0	0	1	0	1	1

b. TD-IDF ->
TF is fraction , IDF is log(N/n) ; N is all documents, and n is times the word appreared in how many documents
TF-IDF : TF * IDF ; this helps to penalize the most common words like a,an,the,is thye etc , because N/n is always 1 , log(1) =0

c. Co-Occurrence Vector ->
1. Co-occurence 
2. Context Window

Corpus = He is not lazy. He is intelligent. He is smart
 	He	is	not	lazy	intelligent	smart
He	0	4	2	1	2	1
is	4	0	1	2	2	1
not	2	1	0	1	0	0
lazy	1	2	1	0	0	0
intelligent	2	2	0	0	0	0
smart	1	1	0	0	0	0

example 1 : My friend Neeraj plays cricket daily Neeraj.
example 2 : He is not lazy. He is intelligent. He is smart

| Example     | Window | Symmetric?            | Count explanation                                                    |
| ----------- | ------ | --------------------- | -------------------------------------------------------------------- |
| My & friend | 2      | Symmetric             | My → friend = 1, friend → My = 1 → total = 2                         |
| He & is     | 2      | Target → context only | He occurs 3 times, context has “is” 4 times → total = 4, not doubled |


the whole sentence is considered on line of tokens where, going up and down is counted (window size : 2 - left or right of the line )
line : He is not lazy. He is intelligent. He is smart

Issues : It requires huge memory to store the co-occurrence matrix.


2. Prediction based embeddings ->
This section assumes that you have a working knowledge of how a neural network works and the mechanisms by which weights in an NN are updated

Word2Vec ->

Word2vec is not a single algorithm but a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model

softmax example ->
Input scores:
[1.0, 2.0, 3.0]
Exponentiate:
[e¹, e², e³] ≈ [2.72, 7.39, 20.09]
Sum:
2.72 + 7.39 + 20.09 = 30.20
Normalized:
[0.09, 0.24, 0.67]
So softmax outputs:
~9%, 24%, 67%

CBOW ->
EX:
| Input  | Output | Hey | This | is | sample | corpus | using | only | one | context | word |
| ------ | ------ | --- | ---- | -- | ------ | ------ | ----- | ---- | --- | ------- | ---- |
| Hey    | this   | 1   | 0    | 0  | 0      | 0      | 0     | 0    | 0   | 0       | 0    |
| this   | hey    | 0   | 1    | 0  | 0      | 0      | 0     | 0    | 0   | 0       | 0    |
| is     | this   | 0   | 0    | 1  | 0      | 0      | 0     | 0    | 0   | 0       | 0    |
| is     | sample | 0   | 0    | 1  | 0      | 0      | 0     | 0    | 0   | 0       | 0    |
| sample | is     | 0   | 0    | 0  | 1      | 0      | 0     | 0    | 0   | 0       | 0    |

(1*10) * (10*4)  -> INPUT LAYER * WEIGHTS ( CONSIDERING ONLY 1 WORD OR ONE-HOT EMBEDDING) -> STEP1
STEP1* OUTPUT WEIGHTS (4*10) -> OUTPUT OF (1*10)   

SIMILARLY IF WE HAVE 3 WORDS -> 3*10 MUL WEIGHTS (10*4) -> 3*4 -> THEN THE AVERAGE IS CALCULATED.


INPUT WEIGHTS (10*4)
OUPUT WEIGHTS (4*10)
AS OUR OUTPUT IS FIXED TO SIZE (1*10)

THIS IS ->
CBOW takes the average of the context of a word (as seen above in calculation of hidden activation). For example, Apple can be both a fruit and a company but CBOW takes an average of both the contexts and places it in between a cluster for fruits and companies.
Training a CBOW from scratch can take forever if not properly optimized.

SKIP - GRAM MODEL ->
WE USE 2 ERROS / OR COMPARE 2 WORDS IN THE SAME CORPUS EX:APPLE , THEN WE GET THE PROBABILTY OF THAT WORD BEING OCCURED IN THE CERTAIN MEANING ?
| Input | Context1 | Context2  |
| ----- | -------- | --------- |
| Hey   | this     | spadding> |
| this  | Hey      | is        |
| is    | this     | sample    |
| ...   | ...      | ...       |

NOTE : Skip-gram predicts multiple context words for each target word (if window size = 2, it predicts 2 context words per target word), 
but fundamentally it's still one target word at a time

| Model     | Input                      | Output                       |
| --------- | -------------------------- | ---------------------------- |
| CBOW      | All context words together | Single target word           |
| Skip-gram | Single target word         | Each context word separately |

CBOW: Uses multiple context words together to predict a single target word.
Skip-gram: Uses a single target word to predict each surrounding context word separately.

EXAMPLE :
Sentence: I love pizza
CBOW:
Input (context): I, pizza → Output (target): love
Skip-gram:
Input (target): love → Outputs (contexts): I, pizza


Important: 
Input  →  (weights W1)  →  Hidden  →  (weights W2)  →  Output

That’s means -  Word2Vec maintains two matrices—input embeddings and output embeddings. 
So if 3000 word sin vocab and 100 weights per each word.
The total parameter is - 3000 * 100 * 2( because one for input and one for output weights

f we have 4 words in a vocab - a b c d
If we want to only predict c using a 
Then we use negative sampling, a(1) and the ret is given (0) to avoid the weight of those words. 
This I turn causes the word2vec to predict less words, example a is used to predict only b and c. 
So this reduces the time/ weights of d in total which helps a lot/saves time

Working of CWOB ->
| Step  | What Happens                               | Example for “Neeraj”                             |
| ----- | ------------------------------------------ | ------------------------------------------------ |
| **1** | Choose target word                         | **Neeraj**                                       |
| **2** | Select context words (2 left + 2 right)    | My, friend, plays, cricket                       |
| **3** | Convert each context word to embeddings    | e(My), e(friend), e(plays), e(cricket)           |
| **4** | Average the embeddings                     | hidden = avg(all 4 embeddings)                   |
| **5** | Predict the target word from hidden vector | Model outputs highest probability for **Neeraj** |
| **6** | Update weights if prediction is wrong      | Improve embeddings for next time                 |

(if we want the embedding size =3 (My → [0.1,0.4,−0.3]) then we have put the activation layer as 3)

Skip -gram :
| Step  | What Happens                                                         | Example for “Neeraj”                                           |
| ----- | -------------------------------------------------------------------- | -------------------------------------------------------------- |
| **1** | Choose **target word**                                               | **Neeraj**                                                     |
| **2** | Identify **context words** (2 left + 2 right)                        | My, friend, plays, cricket                                     |
| **3** | Convert **target word** to its embedding                             | e(Neeraj)                                                      |
| **4** | Use the target embedding to predict **each context word separately** | Predict: My; Predict: friend; Predict: plays; Predict: cricket |
| **5** | Model outputs probability for each context word                      | Highest probs should match the real context words              |
| **6** | Update weights if predictions are wrong                              | Improve e(Neeraj) so it predicts its real neighbors            |


